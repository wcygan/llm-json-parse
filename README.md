# llm-json-parse

HTTP gateway server that enforces JSON schema validation on LLM responses using structured outputs.

## Quick Start

1. **Start LLM Server** (llama-server or compatible):
```bash
llama-server -hf unsloth/gemma-3-4b-it-GGUF:Q4_K_XL
```

2. **Build and Run Gateway**:
```bash
go build -o bin/server ./cmd/server
./bin/server
```

3. **Send Validated Query**:
```bash
curl -X POST http://localhost:8081/v1/validated-query \
  -H "Content-Type: application/json" \
  -d '{
    "schema": {
      "type": "object",
      "properties": {
        "recipe_name": {"type": "string"},
        "ingredients": {
          "type": "array",
          "items": {"type": "string"}
        }
      },
      "required": ["recipe_name", "ingredients"]
    },
    "messages": [
      {"role": "user", "content": "Give me a simple chocolate chip cookie recipe"}
    ]
  }' | jq
```

## Environment Variables

- `LLM_SERVER_URL` - LLM server URL (default: http://localhost:8080)
- `PORT` - Gateway server port (default: 8081)

## Features

- JSON schema validation of LLM responses
- Support for structured outputs via llama-server
- Detailed validation error reporting
- Health check endpoint
- Comprehensive integration test suite with interactive output

## Testing

Run tests with different verbosity levels:

```bash
# Standard test output
make test-integration

# Interactive pretty-printed output
make test-integration-verbose

# All tests with verbose output
make test-all-verbose
```

### Verbose Test Output

When `VERBOSE_TESTS=true`, integration tests display:
- ðŸŽ¨ Colorized JSON with syntax highlighting
- ðŸ“Š Request/response logging with timing
- ðŸ“‹ Schema validation visualization
- âœ… Step-by-step progress tracking
- ðŸ“ˆ Concurrent test performance metrics
